/*
 *  eprop_connection.h
 *
 *  This file is part of NEST.
 *
 *  Copyright (C) 2004 The NEST Initiative
 *
 *  NEST is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  NEST is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with NEST.  If not, see <http://www.gnu.org/licenses/>.
 *
 */

#ifndef EPROP_CONNECTION_H
#define EPROP_CONNECTION_H

// C++ includes:
#include <cmath>

// Includes from nestkernel:
#include "common_synapse_properties.h"
#include "connection.h"
#include "connector_model.h"
#include "event.h"
#include "ring_buffer.h"

// Includes from sli:
#include "dictdatum.h"
#include "dictutils.h"

namespace nest
{

/** @BeginDocumentation
Name: eprop_connection - Synapse type for ... TODO

Description:

clopath_synapse is a connector to create Clopath synapses as defined
in [1]. In contrast to usual STDP, the change of the synaptic weight does
not only depend on the pre- and postsynaptic spike timing but also on the
postsynaptic membrane potential.

Clopath synapses require archiving of continuous quantities. Therefore Clopath
synapses can only be connected to neuron models that are capable of doing this
archiving. So far, compatible models are aeif_psc_delta_clopath and
hh_psc_alpha_clopath.

Parameters:

tau_x    double - Time constant of the trace of the presynaptic spike train.
Wmax     double - Maximum allowed weight.
Wmin     double - Minimum allowed weight.

Other parameters like the amplitudes for depression and facilitation are
stored in in the neuron models that are compatible with the Clopath synapse.

Transmits: SpikeEvent

References:

[1] Clopath et al. (2010) Connectivity reflects coding:
    a model of voltage-based STDP with homeostasis.
    Nature Neuroscience 13:3, 344--352
[2] Clopath and Gerstner (2010) Voltage and spike timing interact
    in STDP â€“ a unified model. Front. Synaptic Neurosci. 2:25
    doi: 10.3389/fnsyn.2010.00025
[3] Voltage-based STDP synapse (Clopath et al. 2010) on ModelDB
    https://senselab.med.yale.edu/ModelDB/showmodel.cshtml?model=144566

Authors: Jonas Stapmanns, David Dahmen, Jan Hahne

SeeAlso: stdp_synapse, aeif_psc_delta_clopath, hh_psc_alpha_clopath
*/
// connections are templates of target identifier type (used for pointer /
// target index addressing) derived from generic connection template
template < typename targetidentifierT >
class EpropConnection : public Connection< targetidentifierT >
{

public:
  typedef CommonSynapseProperties CommonPropertiesType;
  typedef Connection< targetidentifierT > ConnectionBase;

  /**
   * Default Constructor.
   * Sets default values for all parameters. Needed by GenericConnectorModel.
   */
  EpropConnection();


  /**
   * Copy constructor.
   * Needs to be defined properly in order for GenericConnector to work.
   */
  EpropConnection( const EpropConnection& );

  // Explicitly declare all methods inherited from the dependent base
  // ConnectionBase. This avoids explicit name prefixes in all places these
  // functions are used. Since ConnectionBase depends on the template parameter,
  // they are not automatically found in the base class.
  using ConnectionBase::get_delay_steps;
  using ConnectionBase::get_delay;
  using ConnectionBase::get_rport;
  using ConnectionBase::get_target;

  /**
   * Get all properties of this connection and put them into a dictionary.
   */
  void get_status( DictionaryDatum& d ) const;

  /**
   * Set properties of this connection from the values given in dictionary.
   */
  void set_status( const DictionaryDatum& d, ConnectorModel& cm );

  /**
   * Send an event to the receiver of this connection.
   * \param e The event to send
   * \param cp common properties of all synapses (empty).
   */
  void send( Event& e, thread t, const CommonSynapseProperties& cp );


  class ConnTestDummyNode : public ConnTestDummyNodeBase
  {
  public:
    // Ensure proper overriding of overloaded virtual functions.
    // Return values from functions are ignored.
    using ConnTestDummyNodeBase::handles_test_event;
    port
    handles_test_event( SpikeEvent&, rport )
    {
      return invalid_port_;
    }
  };

  void
  check_connection( Node& s,
    Node& t,
    rport receptor_type,
    const CommonPropertiesType& )
  {
    ConnTestDummyNode dummy_target;

    ConnectionBase::check_connection_( dummy_target, s, t, receptor_type );

    if ( EpropArchivingNode* t_eprop = dynamic_cast< EpropArchivingNode* >( &t ) )
    {
      if (t_eprop->is_eprop_readout() )  // if target is a readout neuron
      {
        t_eprop->init_eprop_buffers( 3.0 * get_delay() );
      }
      else
      {
        t_eprop->init_eprop_buffers( 2.0 * get_delay() );
      }
    }
    t.register_stdp_connection( t_lastspike_ - get_delay(), get_delay() );
  }

  void
  set_weight( double w )
  {
    weight_ = w;
  }

private:
  // data members of each connection
  double weight_;
  double learning_rate_;
  double update_interval_;
  double Wmin_;
  double Wmax_;

  double t_lastspike_;
  double t_lastupdate_;
  double t_nextupdate_;
  double last_e_trace_;
  double t_prime_int_trace_;
  double keep_traces_;
  double rate_reg_;
  double target_firing_rate_;
  double tau_low_pass_e_tr_; // time constant for low pass filtering of the eligibility trace
  double propagator_low_pass_; // exp( -dt / tau_low_pass_e_tr_ )
  // TODO: Find a more efficient way to deal with a recall task, i.e. store eprop history only
  // within recall period. This leads to a discontinuous history which needs a different
  // implementation of get_eprop_history, i.e. binary search.

  std::vector< double > pre_syn_spike_times_;
  double batch_size_;
  double m_adam_;  // auxiliary variable for adam optimizer
  double v_adam_;  // auxiliary variable for adam optimizer
  double beta1_adam_;
  double beta2_adam_;
  double epsilon_adam_;
  double recall_duration_;
  std::vector< double > grads_;  // vector that stores the gradients of one batch
  double use_adam_;
};


/**
 * Send an event to the receiver of this connection.
 * \param e The event to send
 * \param t The thread on which this connection is stored.
 * \param cp Common properties object, containing the stdp parameters.
 */
template < typename targetidentifierT >
inline void
EpropConnection< targetidentifierT >::send( Event& e,
  thread t,
  const CommonSynapseProperties& )
{
  double t_spike = e.get_stamp().get_ms();
  // use accessor functions (inherited from Connection< >) to obtain delay and
  // target
  Node* target = get_target( t );
  double dendritic_delay = get_delay();

  // spikes that do not meet the following condition do not need to be delivered because they would
  // arrive during the reset period of the postsynaptic neuron
  if ( ! ( Time( Time::ms( t_spike ) ).get_steps() % Time( Time::ms( update_interval_ ) ).get_steps()
      - Time( Time::ms( dendritic_delay ) ).get_steps() == 0 ) )
  {
    // store times of incoming spikes to enable computation of eligibility trace
    pre_syn_spike_times_.push_back( t_spike );

    // do update only if this is the first spike in a new inverval T
    if ( t_spike > t_nextupdate_ )
    {
      if ( keep_traces_ < 1.0 )
      {
        last_e_trace_ = 0.0;
        t_prime_int_trace_ = 0.0;
      }
      double const dt = Time::get_resolution().get_ms();
      // get spike history in relevant range (t1, t2] from post-synaptic neuron
      std::deque< histentry_eprop >::iterator start;
      std::deque< histentry_eprop >::iterator finish;

      std::deque< double >::iterator start_spk;
      std::deque< double >::iterator finish_spk;
      // DEBUG II: the learning_period_counter corresponds to the variable t of the adam optimizer
      double learning_period_counter_ = floor( ( t_spike - dt ) / update_interval_ )  / batch_size_;
      //DEBUG: added 2*delay to be in sync with TF code
      double t_update_ = ( floor( ( t_spike - dt ) / update_interval_ ) ) * update_interval_ + 2.0 *
        dendritic_delay;
      double grad = 0.0;
      double sum_t_prime_new = 0.0;
      if (target->is_eprop_readout() )  // if target is a readout neuron
      {
        pre_syn_spike_times_.insert( --pre_syn_spike_times_.end(), t_nextupdate_ );
        target->get_eprop_history( pre_syn_spike_times_[0] + dendritic_delay,
            t_lastupdate_ + update_interval_ + dendritic_delay,
            t_lastupdate_ + dendritic_delay,
            t_update_ + dendritic_delay,
            &start,
            &finish );

        std::vector< double > pre_syn_spk_diff(pre_syn_spike_times_.size() - 1);
        std::adjacent_difference(pre_syn_spike_times_.begin(), --pre_syn_spike_times_.end(),
            pre_syn_spk_diff.begin());
        pre_syn_spk_diff.erase( pre_syn_spk_diff.begin() );
        double last_z_hat = 0.0;
        for ( auto pre_syn_spk_t : pre_syn_spk_diff )
        {
          last_z_hat += ( 1.0 - propagator_low_pass_ );
          for (int t = 0; t < pre_syn_spk_t; t++)
          {
            grad += ( ( start->readout_signal_ / start->normalization_ ) - start->target_signal_ ) * last_z_hat;
            ++start;
            last_z_hat *= propagator_low_pass_;
          }
        }
        grad *= dt;
      }
      else  // if target is a neuron of the recurrent network
      {
        pre_syn_spike_times_.insert( --pre_syn_spike_times_.end(), t_nextupdate_ - dendritic_delay );
        target->get_eprop_history( pre_syn_spike_times_[0] + dendritic_delay,
            t_lastupdate_ + update_interval_,
            t_lastupdate_,
            t_update_,
            &start,
            &finish );

        double alpha = target->get_leak_propagator();
        //std::vector< double > elegibility_trace;
        // compute the sum of the elegibility trace because it is used for the firing rate
        // regularization
        double sum_elig_tr = 0.0;
        // compute intervals between presynaptic spikes
        std::vector< double > pre_syn_spk_diff(pre_syn_spike_times_.size() - 1);
        std::adjacent_difference(pre_syn_spike_times_.begin(), --pre_syn_spike_times_.end(),
            pre_syn_spk_diff.begin());
        pre_syn_spk_diff.erase( pre_syn_spk_diff.begin() );
        if ( target->is_eprop_adaptive() )
        {
          // if the target is of type aif_psc_delta_eprop (adaptive threshold)
          double beta = target->get_beta();
          double rho = target->get_adapt_propagator();
          double epsilon = 0.0;
          double last_z_hat = 0.0;
          for ( auto pre_syn_spk_t : pre_syn_spk_diff )
          {
            last_z_hat += 1.0;
            for (int t = 0; t < pre_syn_spk_t; t++)
            {
              //z_hat_new.push_back( last_z_hat );
              double pseudo_deriv = start->V_m_;
              double elig_tr = pseudo_deriv * ( last_z_hat  - beta * epsilon );
              //elegibility_trace.push_back( elig_tr );
              sum_elig_tr += elig_tr;
              epsilon = pseudo_deriv * last_z_hat + ( rho - beta * pseudo_deriv ) * epsilon;
              sum_t_prime_new = propagator_low_pass_ * sum_t_prime_new + ( 1.0 -
                  propagator_low_pass_ ) * elig_tr;
              grad += sum_t_prime_new * dt * ( ( start->readout_signal_ / start->normalization_ ) - start->target_signal_ );
              ++start;
              last_z_hat *= alpha;
            }
          }
        }
        else
        {
          //std::cout << "update synapse" << std::endl;
          //std::cout << "eligibility trace low pass:" << std::endl;
          // if the target is of type iaf_psc_delta_eprop
          double last_z_hat = 0.0;
          for ( auto pre_syn_spk_t : pre_syn_spk_diff )
          {
            last_z_hat += 1.0;
            for ( int t = 0; t < pre_syn_spk_t; t++ )
            {
              //z_hat_new.push_back( last_z_hat );
              double pseudo_deriv = start->V_m_;
              double elig_tr = pseudo_deriv * last_z_hat;
              //std::cout << elig_tr << " | ";
              //elegibility_trace.push_back( elig_tr );
              sum_elig_tr += elig_tr;
              sum_t_prime_new = propagator_low_pass_ * sum_t_prime_new + ( 1.0 -
                  propagator_low_pass_ ) * elig_tr;
              //std::cout << sum_t_prime_new << " , " << ( ( start->readout_signal_ / start->normalization_ ) - start->target_signal_ ) << " | ";
              grad += sum_t_prime_new * dt * ( ( start->readout_signal_ / start->normalization_ ) - start->target_signal_ );
              ++start;
              last_z_hat *= alpha;
            }
          }
          //std::cout << std::endl;
        }
        // firing rate regularization
        target->get_spike_history( t_lastupdate_,
            t_lastupdate_ + update_interval_,
            &start_spk,
            &finish_spk );
        int nspikes = std::distance(start_spk, finish_spk);
        // compute average firing rate since last update. factor 1000 to convert into Hz
        double av_firing_rate = nspikes / update_interval_;
        // Eq.(56)
        grad += rate_reg_ * ( av_firing_rate - target_firing_rate_ / 1000.) * sum_elig_tr * dt /
          update_interval_;
        grad *= dt;
        t_prime_int_trace_ += sum_t_prime_new * dt;
      }
      // implementation of batches: store all gradients in a vector and compute the weight using
      // their mean value.
      grads_.push_back( grad );
      size_t batch_size_cast = batch_size_;
      if ( grads_.size() >= batch_size_cast )
      {
        double sum_grads = 0.0;
        for ( auto gr : grads_ )
        {
          sum_grads += gr;
        }
        if ( use_adam_ == 1.0 ) // use adam optimizer
        {
          // divide also by the number of recall steps to be compatible with the tf implementation
          sum_grads /= Time( Time::ms( recall_duration_ ) ).get_steps() * batch_size_;
          m_adam_ = beta1_adam_ * m_adam_ + ( 1.0 - beta1_adam_ ) * sum_grads;
          v_adam_ = beta2_adam_ * v_adam_ + ( 1.0 - beta2_adam_ ) * std::pow( sum_grads, 2 );
          double alpha_t = learning_rate_ * std::sqrt( 1.0 - std::pow( beta2_adam_, learning_period_counter_ ) )
            / ( 1.0 - std::pow( beta1_adam_, learning_period_counter_ ) );
          weight_ -= alpha_t * m_adam_ / ( std::sqrt( v_adam_ ) + epsilon_adam_ );
        }
        else // gradient descent
        {
          // here we do not divide by the number of recall steps (see tf implementation)
          sum_grads /= batch_size_;
          weight_ -= learning_rate_ * sum_grads;
        }
        grads_.clear();
      }
      // DEBUG: define t_lastupdate_ to be the end of the last period T to be compatible with tf code
      t_lastupdate_ = t_update_;
      t_nextupdate_ += ( floor( ( t_spike - t_nextupdate_ ) / update_interval_ ) + 1 ) *
        update_interval_;
      // clear history of presynaptic spike because we don't need them any more
      pre_syn_spike_times_.clear();
      pre_syn_spike_times_.push_back( t_spike );
      // DEBUG: tidy_eprop_history also takes care of the spike_history
      target->tidy_eprop_history( t_lastupdate_ - dendritic_delay );
    }
  }

  e.set_receiver( *target );
  e.set_weight( weight_ );
  // use accessor functions (inherited from Connection< >) to obtain delay in
  // steps and rport
  e.set_delay_steps( get_delay_steps() );
  e.set_rport( get_rport() );
  e();

  t_lastspike_ = t_spike;
}


template < typename targetidentifierT >
EpropConnection< targetidentifierT >::EpropConnection()
  : ConnectionBase()
  , weight_( 1.0 )
  , learning_rate_( 0.0001 )
  , update_interval_( 1000.0 )
  , Wmin_( 0.0 )
  , Wmax_( 100.0 )
  , t_lastspike_( 0.0 )
  , t_lastupdate_( 0.0 )
  , t_nextupdate_( 100.0 )
  , last_e_trace_( 0.0 )
  , t_prime_int_trace_( 0.0 )
  , keep_traces_( true )
  , rate_reg_( 0. )
  , target_firing_rate_( 10. )
  , tau_low_pass_e_tr_( 0.0 )
  , propagator_low_pass_( 0.0 )
  , batch_size_( 1. )
  , m_adam_( 0.0 )
  , v_adam_( 0.0 )
  , beta1_adam_( 0.9 )
  , beta2_adam_( 0.999 )
  , epsilon_adam_( 1.0e-8 )
  , recall_duration_( 150.0 )  // in ms
  , use_adam_( false )
{
}

template < typename targetidentifierT >
EpropConnection< targetidentifierT >::EpropConnection(
  const EpropConnection< targetidentifierT >& rhs )
  : ConnectionBase( rhs )
  , weight_( rhs.weight_ )
  , learning_rate_( rhs.learning_rate_ )
  , update_interval_( rhs.update_interval_ )
  , Wmin_( rhs.Wmin_ )
  , Wmax_( rhs.Wmax_ )
  , t_lastspike_( rhs.t_lastspike_ )
  , t_lastupdate_( rhs.t_lastupdate_ )
  , t_nextupdate_( rhs.t_nextupdate_ )
  , last_e_trace_( rhs.last_e_trace_ )
  , t_prime_int_trace_( rhs.t_prime_int_trace_ )
  , keep_traces_( rhs.keep_traces_ )
  , rate_reg_( rhs.rate_reg_ )
  , target_firing_rate_( rhs.target_firing_rate_ )
  , tau_low_pass_e_tr_( rhs.tau_low_pass_e_tr_ )
  , propagator_low_pass_( rhs.propagator_low_pass_ )
  , batch_size_( rhs.batch_size_ )
  , m_adam_( rhs.m_adam_ )
  , v_adam_( rhs.v_adam_ )
  , beta1_adam_( rhs.beta1_adam_ )
  , beta2_adam_( rhs.beta2_adam_ )
  , epsilon_adam_( rhs.epsilon_adam_ )
  , recall_duration_( rhs.recall_duration_ )
  , use_adam_( rhs.use_adam_ )
{
}

template < typename targetidentifierT >
void
EpropConnection< targetidentifierT >::get_status( DictionaryDatum& d ) const
{
  ConnectionBase::get_status( d );
  def< double >( d, names::weight, weight_ );
  def< double >( d, names::learning_rate, learning_rate_ );
  def< double >( d, names::update_interval, update_interval_ );
  def< double >( d, names::Wmin, Wmin_ );
  def< double >( d, names::Wmax, Wmax_ );
  def< double >( d, names::keep_traces, keep_traces_ );
  def< double >( d, names::rate_reg, rate_reg_ );
  def< double >( d, names::target_firing_rate, target_firing_rate_ );
  def< double >( d, names::tau_decay, tau_low_pass_e_tr_ );
  def< long >( d, names::size_of, sizeof( *this ) );
  def< double >( d, names::batch_size, batch_size_);
  def< double >( d, names::m_adam, m_adam_);
  def< double >( d, names::v_adam, v_adam_);
  def< double >( d, names::beta1_adam, beta1_adam_);
  def< double >( d, names::beta2_adam, beta2_adam_);
  def< double >( d, names::epsilon_adam, epsilon_adam_);
  def< double >( d, names::recall_duration, recall_duration_);
  def< double >( d, names::use_adam, use_adam_);
}

template < typename targetidentifierT >
void
EpropConnection< targetidentifierT >::set_status( const DictionaryDatum& d,
  ConnectorModel& cm )
{
  ConnectionBase::set_status( d, cm );
  updateValue< double >( d, names::weight, weight_ );
  updateValue< double >( d, names::learning_rate, learning_rate_ );
  updateValue< double >( d, names::update_interval, update_interval_ );
  updateValue< double >( d, names::Wmin, Wmin_ );
  updateValue< double >( d, names::Wmax, Wmax_ );
  updateValue< double >( d, names::keep_traces, keep_traces_ );
  updateValue< double >( d, names::rate_reg, rate_reg_ );
  updateValue< double >( d, names::target_firing_rate, target_firing_rate_ );
  updateValue< double >( d, names::tau_decay, tau_low_pass_e_tr_ );
  updateValue< double >( d, names::batch_size, batch_size_);
  updateValue< double >( d, names::m_adam, m_adam_);
  updateValue< double >( d, names::v_adam, v_adam_);
  updateValue< double >( d, names::beta1_adam, beta1_adam_);
  updateValue< double >( d, names::beta2_adam, beta2_adam_);
  updateValue< double >( d, names::epsilon_adam, epsilon_adam_);
  updateValue< double >( d, names::recall_duration, recall_duration_);
  updateValue< double >( d, names::use_adam, use_adam_);

  const double h = Time::get_resolution().get_ms();
  // TODO: t_nextupdate and t_lastupdate should be initialized even if set_status is not called
  // DEBUG: added + delay to correct for the delay of the learning signal
  t_nextupdate_ = update_interval_ + 2.0 * get_delay(); // TODO: is this waht we want?
  //DEBUG: shifted initial value of t_lastupdate to be in sync with TF code
  t_lastupdate_ = 2.0 * get_delay();
  // compute propagator for low pass filtering of eligibility trace
  if ( tau_low_pass_e_tr_ > 0.0 )
  {
    propagator_low_pass_ = exp( -h / tau_low_pass_e_tr_ );
  }
  else if ( tau_low_pass_e_tr_ == 0.0 )
  {
    propagator_low_pass_ = 0.0;
  }
  else
  {
    throw BadProperty( "The synaptic time constant tau_decay must be greater than zero." );
  }

  // check if weight_ and Wmin_ has the same sign
  if ( not( ( ( weight_ >= 0 ) - ( weight_ < 0 ) )
         == ( ( Wmin_ >= 0 ) - ( Wmin_ < 0 ) ) ) )
  {
    // throw BadProperty( "Weight and Wmin must have same sign." );
  }

  // check if weight_ and Wmax_ has the same sign
  if ( not( ( ( weight_ >= 0 ) - ( weight_ < 0 ) )
         == ( ( Wmax_ > 0 ) - ( Wmax_ <= 0 ) ) ) )
  {
    // throw BadProperty( "Weight and Wmax must have same sign." );
  }

  if ( update_interval_ <= 0.0 )
  {
    throw BadProperty( "The synaptic update interval must be greater than zero." );
  }
}

} // of namespace nest

#endif // of #ifndef URBANCZIK_CONNECTION_H
